---
title: "CPU and GPU Performance"
subtitle: "By Leif Harvey"
format: revealjs
---

## Introduction to `Chips` {.smaller}

* this data set has 2185 CPUs and 2668 GPUs from 2000 till 2020.
* the data comes from Kaggle.com and was last updated in 2022.
* variables of interest:
    * `transistors` is the number of transistors in the chip in millions.
    * `die_size` is the die size used in squared mm.
    * `freq` is the chip's frequency in MHz.
    * `TDP` is the thermal design power measured in watts.
    * `process_size` is the size of the processor in nm.
    * `date` is the chip's release date.
    * `Vendor` is the chip's brand.
    * `Foundry` is the company who produced the chip.
    * `fp16gflops` is a test for the number of 16 bit floating point operations a GPU can do in a second in billions.


```{r}
#| warning: false
#| echo: false
#| output: false

library(ggplot2)
library(tidyverse)
library(plotly)
library(broom)
library(readr)
library(plotly)
library(class)
library(duckdb)
library(DBI)

chips_origional <- read_csv("chip_dataset.csv")

chips <- chips_origional |> rename(transistors = `Transistors (million)`, die_size = `Die Size (mm^2)`, freq = `Freq (MHz)`, TDP = `TDP (W)`, process_size = `Process Size (nm)`, date = `Release Date`, fp16gflops = `FP16 GFLOPS`, fp32gflops = `FP32 GFLOPS`, fp64gflops = `FP64 GFLOPS`)

remove_na <- chips |> filter(!is.na(die_size)) |> filter(!is.na(transistors)) |> filter(!is.na(freq)) |> filter(!is.na(process_size)) |> filter(!is.na(TDP))

chips_nona <- chips |> filter(!is.na(die_size)) |> filter(!is.na(transistors)) |> filter(!is.na(freq)) |> filter(!is.na(process_size)) |> filter(!is.na(TDP)) |> filter(!is.na(Type)) |> filter(!is.na(Foundry))

```

## Foundry Comparision
```{r}
#| warning: false

gpu <- chips |> filter(!is.na(fp32gflops))

foundry_glops <- ggplot(data = gpu, aes(x = Foundry, y = fp32gflops, label = Vendor)) + 
  geom_jitter(alpha = 0.5) + 
  theme_minimal() + 
  ylab("FP32GFLOPS") + 
  labs(title = "Foundry vs FP-32-GFLOPS",
       caption = "GFLOPS represents Billions of Floating Point Operations Per Second")

ggplotly(foundry_glops, tooltip = "label")
```

## Moore's Law
```{r}
#| warning: false

chips$date <- as.Date(chips$date)

foundry_glops <- ggplot(data = chips, aes(x = date, y = transistors, label = date)) + 
  geom_point(alpha = 0.5) + 
  theme_minimal() + 
  ylab("Transistors") + 
  labs(title = "Transistors over Time") +
  scale_x_date(date_labels = "%Y-%m")  # Adjust date format as needed

ggplotly(foundry_glops, tooltip = "label")
```

## Moore's Law CPU
```{r}
#| warning: false

CPU_chips <- chips |> filter(Type == "CPU")

foundry_glops <- ggplot(data = CPU_chips, aes(x = date, y = transistors, label = date)) + 
  geom_point(alpha = 0.5) + 
  theme_minimal() + 
  ylab("Transistors") + 
  labs(title = "CPU Transistors over Time") +
  scale_x_date(date_labels = "%Y-%m")  # Adjust date format as needed

ggplotly(foundry_glops, tooltip = "label")
```

## Moore's Law GPU
```{r}
#| warning: false

GPU_chips <- chips |> filter(Type == "GPU")

foundry_glops <- ggplot(data = GPU_chips, aes(x = date, y = transistors, label = date)) + 
  geom_point(alpha = 0.5) + 
  theme_minimal() + 
  ylab("Transistors") + 
  labs(title = "GPU Transistors over Time") +
  scale_x_date(date_labels = "%Y-%m")  # Adjust date format as needed

ggplotly(foundry_glops, tooltip = "label")
```


## Vendor Prediction {.smaller}

* Used a KNN model with train, test, and validation data to attempt to predict vendor based on 
  * die size (`die_size`)
  * number of transistors (`transistors`)
  * thermal design power (`TDP`) 
  * processor size (`process_size`)
  * frequency (`freq`)

Testing for the best number of neighbors, using $k = 1$ gave the best accuracy rate.

The KNN model only gives an accuracy of $0.713$ which is not that impressive.





